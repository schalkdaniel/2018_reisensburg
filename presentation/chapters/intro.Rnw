\begin{frame}{Why Component-Wise Boosting?}

\vspace{-0.6cm}

\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{images/comp_boosting.png}
\end{figure}

\vspace{-1.2cm}

\begin{itemize}

  \item
    Inherent (unbiased) feature selection  \cite{hofner2011framework}.
    
  \item
    The resulting model is sparse since important effects are selected first and therefore 
    it is able to learn in high-dimensional feature spaces ($p \gg n$).
    
  \item 
    The parameters are updated iteratively. Therefore, the 
    whole trace of how the model evolves is available.    
    
\end{itemize}

\end{frame}



% \begin{frame}[fragile]{Why Component-Wise Boosting?}
% 
% \begin{itemize}
% 
%   \item
%     Inherent (unbiased) feature selection  \cite{hofner2011framework}.
%     
%   \item
%     The resulting model is sparse since important effects are selected first and therefore 
%     it is able to learn in high-dimensional feature spaces ($p \gg n$).
%     
%   \item 
%     The parameters are updated iteratively. Therefore, the 
%     whole trace of how the model evolves is available.    
%     
% \end{itemize}
% 
% \end{frame}



% \begin{frame}{Component-Wise Boosting: The Algorithm}

% \begin{Shaded}
% \begin{algorithm}[H]
% % \KwData{this text}
% \scriptsize
% \KwResult{Component-wise boosting model $\fh(x)$}
% Initialize $\fh^{[0]}(x) = \argmin_{c\in\R} \riske(c)$ \;
%   \For{$m \in \{1, \dots, M\}$}{\vspace{0.2cm}
%     // Update pseudo residuals: \\
%     $r^{[m](i)} = -\left[ \frac{\delta}{\delta f(\xi)} L\left(\yi, f(\xi)\right) \right]_{f = f^{[m-1]}},\ \forall i \in \{1, \dots, n\}$ \;\vspace{0.2cm}
%     // Get index $j^\ast$ of $m$-th base-learner from optimizer:\\
%     \For{$j \in \{1, \dots, J\}$}{
%       // Fit each base-learner $b_j^{[m]}$ to the pseudo residuals: \\
%       $\hat{\theta}_j^{[m]} = \argmin_{\theta_j} \sum\limits_{i=1}^n\left( 
%       \rmi - b_j^{[m]}(\xi, \theta_j)\right)^2$ \;\vspace{0.2cm}
%       // Calculate the SSE of the fitted base-learner:\\
%       $\mathsf{SSE}_j = \sum\limits_{i=1}^n \left(\rmi - b_j^{[m]}(\xi, \hat{\theta}_j)\right)^2$ \; 
%     }
%     // Add selected component to model:\\
%     $\fmh(x) = \fmdh(x) + \beta b^{[m]}_{j^\ast}\left(x, \theta_{j^\ast}^{[m]}\right)$
%   }
% \textbf{Returns:} $\fh(x) = \fmh(x)$\;
% \end{algorithm}
% \end{Shaded}

% \end{frame}


\begin{frame}[fragile]{Available R Packages}

Most popular package for model-based boosting is \texttt{mboost} \parbox{1cm}{\parbox{4cm}{\cite{mboost1}:}}

\begin{itemize}
	
  \item
    Large number of available base-learner and losses.

  \item 
    Extended to more complex problems:
    \begin{itemize}
      \item Functional data \cite{brockhaus2017boosting}
      \item GAMLSS models \cite{mayr2012generalized}
      \item Survival analysis
    \end{itemize}
  \item 
    Extendible with custom base-learner and losses. 

\end{itemize}

\textbf{So, why another boosting implementation?}

\begin{itemize}

  \item
    The main parts of \texttt{mboost} are written in \texttt{R} and gets slow for large datasets.

  \item 
    Complex implementation:
    \begin{itemize} 
      \item Nested scopes 
      \item Mixture of different \texttt{R} class systems
    \end{itemize}
\end{itemize}

\end{frame}
