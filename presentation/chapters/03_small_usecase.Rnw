\begin{frame}[fragile]{Initialize Model}

We are interested in modelling the risk of diabetes of female Pima Indians. Interesting features are \texttt{age} and the \texttt{mass}. 

<<eval=FALSE>>=
library(compboost)

data(PimaIndiansDiabetes, package = "mlbench")

# Defining a new Compboost object:
cboost = Compboost$new(data = PimaIndiansDiabetes, target = "diabetes", 
  loss = LossBinomial$new())

# Adding a linear and spline base-learner to the Compboost object:
cboost$addBaselearner(feature = "mass", id = "linear", BaselearnerPolynomial, 
  degree = 1, intercept = TRUE)
cboost$addBaselearner(feature = "age", id = "spline", BaselearnerPSpline, 
  degree = 3, n.knots = 10, penalty = 2, differences = 2)
@

<<include=FALSE>>=
library(mboost)

data(PimaIndiansDiabetes, package = "mlbench")

# Defining a new Compboost object:
cboost = Compboost$new(data = PimaIndiansDiabetes, target = "diabetes", 
  loss = LossBinomial$new())

# Adding a linear and spline base-learner to the Compboost object:
cboost$addBaselearner(feature = "mass", id = "linear", BaselearnerPolynomial, 
  degree = 1, intercept = TRUE)
cboost$addBaselearner(feature = "age", id = "spline", BaselearnerPSpline, 
  degree = 3, n.knots = 10, penalty = 2, differences = 2)
@


\end{frame}

\begin{frame}[fragile]{Initialize Model}

<<>>=
cboost$train(2000, trace = 1000) 
cboost
@


\end{frame}

\begin{frame}[fragile]{Access Results and Continue Training}

<<>>=
cboost$train(1000) # Set model to iteration 1000

table(cboost$getSelectedBaselearner()) # Table of vector of selected base-learner
cboost$train(3000) # Set model to iteration 3000
str(cboost$getInbagRisk()) # Get vector of inbag risk
str(cboost$getEstimatedCoef()) # Get list of estimated parameter
@

\end{frame}


\begin{frame}[fragile]{Plot Results}

<<eval=FALSE>>=
cboost$plot("age_spline", iters = c(100, 500, 1000, 2000, 3000))
@

\begin{center}
<<out.width="11cm",out.height="6cm",echo=FALSE,fig.asp=0.5455,warning=FALSE,message=FALSE>>=
library(ggplot2)
library(ggthemes)

cboost$plot("age_spline", iters = c(100, 500, 1000, 2000, 3000)) +
	theme_tufte() + 
	scale_color_brewer(palette = "Spectral")
@
\end{center}

\end{frame}

\begin{frame}[fragile]{Custom Loss: Definition}

As an example we want to define a custom loss corresponding to the Poisson distribution:

<<>>=
lossPoisson = function (truth, response) {
  return (-log(exp(response)^truth * exp(-exp(response)) / gamma(truth + 1)))
}
gradPoisson = function (truth, response) {
  return (exp(response) - truth)
}
constInitPoisson = function (truth) {
  return (log(mean.default(truth)))
}
# Define custom loss:
my.poisson.loss = LossCustom$new(lossPoisson, gradPoisson, constInitPoisson)
@

\end{frame}


\begin{frame}[fragile]{Custom Loss: Train Model}

<<>>=
data(VonBort, package = "vcd")

# Run compboost with custom loss:
cboost = Compboost$new(VonBort, "deaths", loss = my.poisson.loss)
cboost$addBaselearner("year", "spline", BaselearnerPSpline)
cboost$train(100, trace = 0)

# Run mboost with pre-defined Poisson family:
mod = mboost(deaths ~ bbs(year, lambda = 2), data = VonBort, family = Poisson(), 
  control = boost_control(mstop = 100, nu = 0.05))

head(data.frame(
  compboost = cboost$getEstimatedCoef()[["year_spline"]], 
  mboost = coef(mod)[["bbs(year, lambda = 2)"]]))
@

\end{frame}

\begin{frame}[fragile]{Using Wrapper Functions}

Instead of defining all base-learner individually we can use \texttt{boostLinear()} or \texttt{boostSplines()} to boost just linear or spline base-learner on all features:

<<>>=
# Run wrapper again with custom loss:
cboost = boostSplines(data = VonBort, target = "deaths", loss = my.poisson.loss, 
  trace = 50, learning.rate = 0.01, iterations = 200)

head(cboost$getBaselearnerNames())
@

\end{frame}